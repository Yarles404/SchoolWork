---
title: "Proj2 Report"
author: "Charles Yang"
date: "9/29/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(readxl)
```

# Data
```{r echo=FALSE}
sortdata <- readxl::read_xlsx("SortData.xlsx")
print.data.frame(sortdata)
```

# Brief Introduction and Notes
Insertion, Merge, and Quick Sort were all run to sort 1 million total words, at varying list sizes. Total time, average time per sort, words per minute, and total comparisons made were noted. Each size had it's own unique list and alphabet, but were used consistently accross all Sorters.
Note: Insertion sort was not run for list sizes n >= 100,000. At n = 10,000, the program took nearly an hour to complete. Merge and Quick sort ran within a minute in all cases, so they were ran at greater list sizes.
Note: Anywhere in this document where a "graph" is mentioned is meant to be accompanied by actual graphs in a separate document. If you aren't compelled to look at the corresponding HTML document, you can interpret graph as "pattern discovered by analyzing data."

All data is stored SortData.xlsx.

# Time Analysis
```{r echo=FALSE}
insertion <- drop_na(filter(sortdata, sorter == "Insertion"))
ggplot(insertion, aes(x=n, y=time)) +
  geom_line() +
  geom_point() +
  scale_x_continuous(trans="log10") +
  ylab("Time in ms") +
  ggtitle("Time Taken to Insertion Sort 1 mil Total Words v. List Size")
  
```

```{r echo=FALSE}
merge <- drop_na(filter(sortdata, sorter == "Merge"))
ggplot(merge, aes(x=n, y=time)) +
  geom_line() +
  geom_point() +
  scale_x_continuous(trans="log10") +
  ylab("Time in ms") +
  ggtitle("Time to Merge Sort 1 mil Total Words v. List Size")
  
```

```{r echo=FALSE}
quick <- drop_na(filter(sortdata, sorter == "Quick"))
ggplot(quick, aes(x=n, y=time)) +
  geom_line() +
  geom_point() +
  scale_x_continuous(trans="log10") +
  ylab("Time in ms") +
  ggtitle("Time to Quick Sort 1 mil Total Words v. List Size")
```

Insertion Sort's graph for list size v. time appears to be exponential/polynomial or some other higher degree despite base-10 logarithmically transforming the x-axis. This is consistent with the expected worst case scenario of O(n^2)
Merge and Quick Sort's graphs appear the same and linear when transforming the x-axis logarthmically. This implies that the algorithms are roughly O(logn). This is somewhat consistent with Merge Sort's expected O(nlogn). However, Quicksort has a worst case O(n^2), but is a very rare case. Perhaps this accounts for the slight difference in sort times, average times per sort, and comparison's per second between Merge and Quick Sort.

Insertion Sorter's time per list increases exponentially, while Merge and Quick Sort's time per list increases exponentially, but to a lesser degree.

# Total Comparisons Analysis# Average Sort Time and Words Per Minute
Average Sort Time and Words Per Minute follow a similar analysis.
Insertion sorter, sorts exponentially less words per minute as n increases, while Merge and Quick Sort sort also exponentialy less words, but to a much lesser degree. This is consistent with their expected runtimes with respect to list size.

```{r echo=FALSE}
insertion <- drop_na(filter(sortdata, sorter == "Insertion"))
ggplot(insertion, aes(x=n, y=total_comps)) +
  geom_line() +
  geom_point() +
  scale_x_continuous(trans="log10") +
  scale_y_continuous(trans="log10") +
  ylab("Total Comparisons") +
  ggtitle("Insertion Sort Total Comparisons v. List Size")
```

```{r echo=FALSE}
merge <- drop_na(filter(sortdata, sorter == "Merge"))
ggplot(merge, aes(x=n, y=total_comps)) +
  geom_line() +
  geom_point() +
  scale_x_continuous(trans="log10") +
  ylab("Total Comparisons") +
  ggtitle("Merge Sort Total Comparisons v. List Size")
```

```{r echo=FALSE}
quick <- drop_na(filter(sortdata, sorter == "Quick"))
ggplot(quick, aes(x=n, y=total_comps)) +
  geom_line() +
  geom_point() +
  scale_x_continuous(trans="log10") +
  ylab("Total Comparisons") +
  ggtitle("Quick Sort Total Comparisons v. List Size")
```

Quick Sort's total comparisons seem to increase tenfold as you mutliply the list size by 10. The graph demonstrates this when transforming n and total comparisons by base-10 logarithm.
Merge and Quick sort's total comparison's appear linear when transforming list size logarthmically. As you multiply the list size by 10, we have a linear increase in total comparisons.
Quick Sort has notably more comparisons that Merge Sort, which may account for the higher sort time.